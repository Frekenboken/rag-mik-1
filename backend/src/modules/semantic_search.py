import faiss
import re
import string
from natasha import MorphVocab

from src.modules.rerank import Rerank
from src.modules.text_embedder import TextEmbedder
from difflib import SequenceMatcher
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

import nltk
# –°–∫–∞—á–∞–π—Ç–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_rus')

# –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è
# nltk.download('rslp')  # –¥–ª—è —Å—Ç–µ–º–º–∏–Ω–≥–∞


class SemanticSearch:
    def __init__(self, vector_db: faiss.swigfaiss_avx2.IndexFlatIP, chunks, rerank: Rerank, embedding: TextEmbedder):
        self.vector_db = vector_db
        self.chunks = chunks
        self.rerank = rerank
        self.embedding = embedding
        self.morph = MorphVocab()

    def search_debuging(self, query_text, k=3):
        relevance = self.rerank.reranking_and_format(
            self.vector_db.search(self.embedding.get_embeddings([query_text.lower()]), k)[1][0],
            query_text.lower(),
            self.chunks
            )
        print(f"üîç –ó–∞–ø—Ä–æ—Å: '{query_text}'\n")
        for i, (similarity, idx) in enumerate(relevance):
            print(f"{i+1}.")
            print(f"{self.chunks[idx][0]}")
            print(f"   üìä –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}\n")
            print("=" * 80)

    def search(self, query_text, k=3):
         relevance = self.rerank.reranking_and_format(
            self.vector_db.search(self.embedding.get_embeddings([query_text.lower()]), k)[1][0],
            query_text.lower(),
            self.chunks
            )
         # return [self.chunks[idx] for similarity, idx in filter(lambda x: x[0] > 0.17, relevance)]
         return [self.chunks[idx] for similarity, idx in relevance if similarity > 0.17]

    def extract_keywords(self, text):

        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é string.punctuation
        cleaned_text = ''.join(char for char in text if char not in string.punctuation)

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤
        words = word_tokenize(cleaned_text)

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ NLTK
        stop_words = set(stopwords.words('russian'))

        # –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        filtered_words = [word for word in words if word.lower() not in stop_words]

        # –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
        lemmatized_words = [self.morph.parse(word)[0].normal_form for word in filtered_words]

        tagged_lemmatized_words = pos_tag(lemmatized_words, lang='rus')

        return [i[0] for i in filter(lambda x: x[1] == 'S', tagged_lemmatized_words)]
        #return tagged_lemmatized_words

    def calculate_similarity(self, answer: str, expected: str) -> float:
        """
        –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        answer = answer.lower().strip()
        expected = expected.lower().strip()

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–µ–ª –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        numbers_answer = set(re.findall(r'\d+(?:\.\d+)?', answer))
        numbers_expected = set(re.findall(r'\d+(?:\.\d+)?', expected))

        # –ï—Å–ª–∏ –µ—Å—Ç—å —á–∏—Å–ª–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if numbers_expected:
            numbers_match = len(numbers_answer & numbers_expected) / len(numbers_expected)
        else:
            numbers_match = 1.0

        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        text_similarity = SequenceMatcher(None, answer, expected).ratio()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.extract_keywords(expected)
        if keywords:
            keywords_found = sum(1 for kw in keywords if kw in answer) / len(keywords)
        else:
            keywords_found = 1.0

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        final_score = (numbers_match * 0.4 + text_similarity * 0.3 + keywords_found * 0.3)

        return final_score